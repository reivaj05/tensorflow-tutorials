{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Download mnist data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model inputs\n",
    "\n",
    "def model_inputs(discriminator_dim, generator_dim):\n",
    "    discriminator_inputs = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, discriminator_dim],\n",
    "        name='discriminator_input'\n",
    "    )\n",
    "    generator_inputs = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, generator_dim],\n",
    "        name='generator_input'\n",
    "    )\n",
    "    return discriminator_inputs, generator_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create generator\n",
    "\n",
    "def generator(_input, out_dim, num_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        hidden_layer = tf.layers.dense(_input, num_units)\n",
    "        leaky_relu = tf.maximum(hidden_layer * alpha, hidden_layer)\n",
    "        \n",
    "        logits = tf.layers.dense(leaky_relu, out_dim)\n",
    "        return tf.tanh(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create discriminator\n",
    "\n",
    "def discriminator(_input, num_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        hidden_layer = tf.layers.dense(_input, num_units)\n",
    "        leaky_relu = tf.maximum(hidden_layer * alpha, hidden_layer)\n",
    "        \n",
    "        logits = tf.layers.dense(leaky_relu, 1)\n",
    "        return tf.sigmoid(logits), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "discriminator_input_size = 784\n",
    "generator_input_size = 100\n",
    "\n",
    "generator_hidden_size = discriminator_hidden_size = 128\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "learning_rate = 0.002\n",
    "\n",
    "smooth = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the network\n",
    "\n",
    "tf.reset_default_graph()\n",
    "## Create input placeholders\n",
    "discriminator_input, generator_input = model_inputs(\n",
    "    discriminator_input_size, generator_input_size)\n",
    "\n",
    "\n",
    "## Build the model\n",
    "\n",
    "generator_model = generator(generator_input, discriminator_input_size)\n",
    "\n",
    "discriminator_model_real, discriminator_logits_real = discriminator(discriminator_input)\n",
    "discriminator_model_fake, discriminator_logits_fake = discriminator(generator_model, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Losses\n",
    "\n",
    "## Discriminator loss real\n",
    "\n",
    "discriminator_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=discriminator_logits_real,\n",
    "    labels=tf.ones_like(discriminator_logits_real) * (1 - smooth)\n",
    ")\n",
    "discriminator_loss_real = tf.reduce_mean(discriminator_loss_real)\n",
    "\n",
    "## Discriminator loss fake\n",
    "\n",
    "discriminator_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=discriminator_logits_fake,\n",
    "    labels=tf.zeros_like(discriminator_logits_real)\n",
    ")\n",
    "discriminator_loss_fake = tf.reduce_mean(discriminator_loss_fake)\n",
    "\n",
    "## Discriminator loss\n",
    "\n",
    "discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "\n",
    "## Generator loss\n",
    "\n",
    "generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=discriminator_logits_fake,\n",
    "    labels=tf.ones_like(discriminator_logits_fake)\n",
    ")\n",
    "\n",
    "generator_loss = tf.reduce_mean(generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "## Get the trainable variables to split into generator and discriminator vars\n",
    "train_variables = tf.trainable_variables()\n",
    "\n",
    "generator_variables = [var for var in train_variables if var.name.startswith('generator')]\n",
    "discriminator_variables = [var for var in train_variables if var.name.startswith('discriminator')]\n",
    "\n",
    "## Create optimizers with var lists\n",
    "\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "    generator_loss, var_list=generator_variables)\n",
    "\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "    discriminator_loss, var_list=discriminator_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch 1/100...', 'Discriminator Loss: 0.3610...', 'Generator Loss: 3.7139')\n",
      "('Epoch 2/100...', 'Discriminator Loss: 0.3537...', 'Generator Loss: 3.9365')\n",
      "('Epoch 3/100...', 'Discriminator Loss: 0.4888...', 'Generator Loss: 3.7891')\n",
      "('Epoch 4/100...', 'Discriminator Loss: 0.4025...', 'Generator Loss: 3.9308')\n",
      "('Epoch 5/100...', 'Discriminator Loss: 0.9150...', 'Generator Loss: 3.6636')\n",
      "('Epoch 6/100...', 'Discriminator Loss: 1.5940...', 'Generator Loss: 1.7054')\n",
      "('Epoch 7/100...', 'Discriminator Loss: 1.1450...', 'Generator Loss: 5.1282')\n",
      "('Epoch 8/100...', 'Discriminator Loss: 1.2245...', 'Generator Loss: 1.8742')\n",
      "('Epoch 9/100...', 'Discriminator Loss: 1.0641...', 'Generator Loss: 1.2291')\n",
      "('Epoch 10/100...', 'Discriminator Loss: 1.0782...', 'Generator Loss: 2.1048')\n",
      "('Epoch 11/100...', 'Discriminator Loss: 1.0230...', 'Generator Loss: 2.1220')\n",
      "('Epoch 12/100...', 'Discriminator Loss: 0.8871...', 'Generator Loss: 2.4581')\n",
      "('Epoch 13/100...', 'Discriminator Loss: 1.0302...', 'Generator Loss: 2.2410')\n",
      "('Epoch 14/100...', 'Discriminator Loss: 1.3629...', 'Generator Loss: 1.5072')\n",
      "('Epoch 15/100...', 'Discriminator Loss: 1.4571...', 'Generator Loss: 2.5060')\n",
      "('Epoch 16/100...', 'Discriminator Loss: 1.3771...', 'Generator Loss: 1.5331')\n",
      "('Epoch 17/100...', 'Discriminator Loss: 2.1457...', 'Generator Loss: 0.8336')\n",
      "('Epoch 18/100...', 'Discriminator Loss: 0.8136...', 'Generator Loss: 2.1608')\n",
      "('Epoch 19/100...', 'Discriminator Loss: 1.0390...', 'Generator Loss: 1.8266')\n",
      "('Epoch 20/100...', 'Discriminator Loss: 1.2297...', 'Generator Loss: 1.6069')\n",
      "('Epoch 21/100...', 'Discriminator Loss: 1.1496...', 'Generator Loss: 1.4269')\n",
      "('Epoch 22/100...', 'Discriminator Loss: 0.8517...', 'Generator Loss: 2.0660')\n",
      "('Epoch 23/100...', 'Discriminator Loss: 0.8255...', 'Generator Loss: 1.9235')\n",
      "('Epoch 24/100...', 'Discriminator Loss: 1.1326...', 'Generator Loss: 1.8619')\n",
      "('Epoch 25/100...', 'Discriminator Loss: 1.0850...', 'Generator Loss: 2.3271')\n",
      "('Epoch 26/100...', 'Discriminator Loss: 0.9823...', 'Generator Loss: 2.0439')\n",
      "('Epoch 27/100...', 'Discriminator Loss: 0.8183...', 'Generator Loss: 2.3446')\n",
      "('Epoch 28/100...', 'Discriminator Loss: 0.9516...', 'Generator Loss: 1.6764')\n",
      "('Epoch 29/100...', 'Discriminator Loss: 0.9301...', 'Generator Loss: 2.0445')\n",
      "('Epoch 30/100...', 'Discriminator Loss: 0.8673...', 'Generator Loss: 1.9762')\n",
      "('Epoch 31/100...', 'Discriminator Loss: 1.1893...', 'Generator Loss: 1.4226')\n",
      "('Epoch 32/100...', 'Discriminator Loss: 0.8114...', 'Generator Loss: 2.3016')\n",
      "('Epoch 33/100...', 'Discriminator Loss: 1.0492...', 'Generator Loss: 1.8329')\n",
      "('Epoch 34/100...', 'Discriminator Loss: 0.9259...', 'Generator Loss: 2.8954')\n",
      "('Epoch 35/100...', 'Discriminator Loss: 1.0282...', 'Generator Loss: 2.0221')\n",
      "('Epoch 36/100...', 'Discriminator Loss: 0.8891...', 'Generator Loss: 2.0999')\n",
      "('Epoch 37/100...', 'Discriminator Loss: 0.8232...', 'Generator Loss: 2.6474')\n",
      "('Epoch 38/100...', 'Discriminator Loss: 1.0443...', 'Generator Loss: 2.1272')\n",
      "('Epoch 39/100...', 'Discriminator Loss: 0.9505...', 'Generator Loss: 2.1103')\n",
      "('Epoch 40/100...', 'Discriminator Loss: 0.7978...', 'Generator Loss: 2.3140')\n",
      "('Epoch 41/100...', 'Discriminator Loss: 1.0642...', 'Generator Loss: 1.9769')\n",
      "('Epoch 42/100...', 'Discriminator Loss: 0.8561...', 'Generator Loss: 2.2247')\n",
      "('Epoch 43/100...', 'Discriminator Loss: 1.0182...', 'Generator Loss: 2.0321')\n",
      "('Epoch 44/100...', 'Discriminator Loss: 0.9855...', 'Generator Loss: 1.5972')\n",
      "('Epoch 45/100...', 'Discriminator Loss: 1.4146...', 'Generator Loss: 1.1930')\n",
      "('Epoch 46/100...', 'Discriminator Loss: 1.2144...', 'Generator Loss: 2.7811')\n",
      "('Epoch 47/100...', 'Discriminator Loss: 0.8786...', 'Generator Loss: 2.1741')\n",
      "('Epoch 48/100...', 'Discriminator Loss: 1.0341...', 'Generator Loss: 1.9743')\n",
      "('Epoch 49/100...', 'Discriminator Loss: 1.1616...', 'Generator Loss: 1.8697')\n",
      "('Epoch 50/100...', 'Discriminator Loss: 0.9131...', 'Generator Loss: 1.8982')\n",
      "('Epoch 51/100...', 'Discriminator Loss: 1.1300...', 'Generator Loss: 1.8999')\n",
      "('Epoch 52/100...', 'Discriminator Loss: 1.0230...', 'Generator Loss: 1.7355')\n",
      "('Epoch 53/100...', 'Discriminator Loss: 0.9176...', 'Generator Loss: 1.6912')\n",
      "('Epoch 54/100...', 'Discriminator Loss: 0.8761...', 'Generator Loss: 2.3600')\n",
      "('Epoch 55/100...', 'Discriminator Loss: 0.9307...', 'Generator Loss: 1.7238')\n",
      "('Epoch 56/100...', 'Discriminator Loss: 0.7768...', 'Generator Loss: 2.4752')\n",
      "('Epoch 57/100...', 'Discriminator Loss: 0.9234...', 'Generator Loss: 2.0741')\n",
      "('Epoch 58/100...', 'Discriminator Loss: 0.9194...', 'Generator Loss: 1.7478')\n",
      "('Epoch 59/100...', 'Discriminator Loss: 0.9222...', 'Generator Loss: 1.8859')\n",
      "('Epoch 60/100...', 'Discriminator Loss: 1.0376...', 'Generator Loss: 1.8256')\n",
      "('Epoch 61/100...', 'Discriminator Loss: 0.9389...', 'Generator Loss: 2.2363')\n",
      "('Epoch 62/100...', 'Discriminator Loss: 1.0775...', 'Generator Loss: 1.8134')\n",
      "('Epoch 63/100...', 'Discriminator Loss: 1.0164...', 'Generator Loss: 1.4308')\n",
      "('Epoch 64/100...', 'Discriminator Loss: 0.9118...', 'Generator Loss: 2.2420')\n",
      "('Epoch 65/100...', 'Discriminator Loss: 0.9385...', 'Generator Loss: 1.8752')\n",
      "('Epoch 66/100...', 'Discriminator Loss: 0.8307...', 'Generator Loss: 1.9120')\n",
      "('Epoch 67/100...', 'Discriminator Loss: 0.8071...', 'Generator Loss: 2.5180')\n",
      "('Epoch 68/100...', 'Discriminator Loss: 0.9992...', 'Generator Loss: 2.0578')\n",
      "('Epoch 69/100...', 'Discriminator Loss: 1.0193...', 'Generator Loss: 2.0279')\n",
      "('Epoch 70/100...', 'Discriminator Loss: 0.8511...', 'Generator Loss: 1.7309')\n",
      "('Epoch 71/100...', 'Discriminator Loss: 0.9783...', 'Generator Loss: 1.8921')\n",
      "('Epoch 72/100...', 'Discriminator Loss: 0.9808...', 'Generator Loss: 1.8034')\n",
      "('Epoch 73/100...', 'Discriminator Loss: 0.9443...', 'Generator Loss: 1.7591')\n",
      "('Epoch 74/100...', 'Discriminator Loss: 0.9698...', 'Generator Loss: 1.5257')\n",
      "('Epoch 75/100...', 'Discriminator Loss: 0.9707...', 'Generator Loss: 1.6312')\n",
      "('Epoch 76/100...', 'Discriminator Loss: 0.8781...', 'Generator Loss: 2.6984')\n",
      "('Epoch 77/100...', 'Discriminator Loss: 0.9168...', 'Generator Loss: 1.9443')\n",
      "('Epoch 78/100...', 'Discriminator Loss: 0.9764...', 'Generator Loss: 1.8984')\n",
      "('Epoch 79/100...', 'Discriminator Loss: 0.9344...', 'Generator Loss: 1.8933')\n",
      "('Epoch 80/100...', 'Discriminator Loss: 1.1613...', 'Generator Loss: 1.9193')\n",
      "('Epoch 81/100...', 'Discriminator Loss: 1.0227...', 'Generator Loss: 1.7076')\n",
      "('Epoch 82/100...', 'Discriminator Loss: 0.8183...', 'Generator Loss: 1.8793')\n",
      "('Epoch 83/100...', 'Discriminator Loss: 0.9112...', 'Generator Loss: 1.8770')\n",
      "('Epoch 84/100...', 'Discriminator Loss: 0.9264...', 'Generator Loss: 2.1068')\n",
      "('Epoch 85/100...', 'Discriminator Loss: 0.8174...', 'Generator Loss: 2.0643')\n",
      "('Epoch 86/100...', 'Discriminator Loss: 0.8852...', 'Generator Loss: 2.1594')\n",
      "('Epoch 87/100...', 'Discriminator Loss: 1.0170...', 'Generator Loss: 1.5330')\n",
      "('Epoch 88/100...', 'Discriminator Loss: 0.9006...', 'Generator Loss: 1.9815')\n",
      "('Epoch 89/100...', 'Discriminator Loss: 1.0992...', 'Generator Loss: 1.5161')\n",
      "('Epoch 90/100...', 'Discriminator Loss: 1.0275...', 'Generator Loss: 1.7076')\n",
      "('Epoch 91/100...', 'Discriminator Loss: 0.8108...', 'Generator Loss: 1.6832')\n",
      "('Epoch 92/100...', 'Discriminator Loss: 0.9183...', 'Generator Loss: 1.8923')\n",
      "('Epoch 93/100...', 'Discriminator Loss: 1.0284...', 'Generator Loss: 2.6486')\n",
      "('Epoch 94/100...', 'Discriminator Loss: 0.8037...', 'Generator Loss: 1.9870')\n",
      "('Epoch 95/100...', 'Discriminator Loss: 0.9714...', 'Generator Loss: 1.8809')\n",
      "('Epoch 96/100...', 'Discriminator Loss: 0.9094...', 'Generator Loss: 1.9679')\n",
      "('Epoch 97/100...', 'Discriminator Loss: 0.8892...', 'Generator Loss: 1.7226')\n",
      "('Epoch 98/100...', 'Discriminator Loss: 1.0335...', 'Generator Loss: 1.8607')\n",
      "('Epoch 99/100...', 'Discriminator Loss: 0.8608...', 'Generator Loss: 1.9716')\n",
      "('Epoch 100/100...', 'Discriminator Loss: 0.9031...', 'Generator Loss: 1.9182')\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "# Only save generator variables\n",
    "saver = tf.train.Saver(var_list=generator_variables)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(mnist.train.num_examples // batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Get images, reshape and rescale to pass to D\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images * 2 - 1\n",
    "            \n",
    "            # Sample random noise for generator\n",
    "            batch_generator = np.random.uniform(-1, 1, size=(batch_size, generator_input_size))\n",
    "            \n",
    "            # Run optimizers\n",
    "            session.run(\n",
    "                discriminator_optimizer,\n",
    "                feed_dict={\n",
    "                    discriminator_input: batch_images,\n",
    "                    generator_input: batch_generator,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            session.run(\n",
    "                generator_optimizer,\n",
    "                feed_dict={generator_input: batch_generator}\n",
    "            )\n",
    "            \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_discriminator = session.run(\n",
    "            discriminator_loss,\n",
    "            feed_dict={\n",
    "                discriminator_input: batch_images,\n",
    "                generator_input: batch_generator,\n",
    "            }\n",
    "        )\n",
    "        train_loss_generator = generator_loss.eval({generator_input: batch_generator})\n",
    "        \n",
    "        print(\"Epoch {}/{}...\".format(epoch + 1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_discriminator),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_generator))\n",
    "        \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_discriminator, train_loss_generator))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_generator = np.random.uniform(-1, 1, size=(16, generator_input_size))\n",
    "        generator_samples = session.run(\n",
    "            generator(generator_input, discriminator_input_size, reuse=True),\n",
    "            feed_dict={generator_input: sample_generator}\n",
    "        )\n",
    "        samples.append(generator_samples)\n",
    "        saver.save(session, './checkpoints/generator.ckpt')\n",
    "        \n",
    "with open('train_samples.pkl', 'wb') as _file:\n",
    "    pkl.dump(samples, _file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
