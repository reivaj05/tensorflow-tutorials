{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Semisupervised learning GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "# There are two ways of solving this problem.\n",
    "# One is to have the matmul at the last layer output all 11 classes.\n",
    "# The other is to output just 10 classes, and use a constant value of 0 for\n",
    "# the logit for the last class. This still works because the softmax only needs\n",
    "# n independent logits to specify a probability distribution over n + 1 categories.\n",
    "# We implemented both solutions here.\n",
    "extra_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "\n",
    "if not isdir(data_dir):\n",
    "    raise Exception('Data directory does not exist')\n",
    "    \n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    \n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "        \n",
    "if not isfile(data_dir + 'train_32x32.mat'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Training set') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
    "            data_dir + 'train_32x32.mat',\n",
    "            pbar.hook\n",
    "        )\n",
    "        \n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Test set') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
    "            data_dir + 'test_32x32.mat',\n",
    "            pbar.hook\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainset = loadmat(data_dir + 'train_32x32.mat')\n",
    "testset = loadmat(data_dir + 'test_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x, feature_range=(-1, 1)):\n",
    "    x = ((x - x.min()) / (255 - x.min()))\n",
    "    \n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dataset:\n",
    "    def __init__(self, train, test, val_frac=0.5, shuffle=True, scale_func=None):\n",
    "        split_idx = int(len(test['y'])*(1 - val_frac))\n",
    "        self.test_x, self.valid_x = test['X'][:,:,:,:split_idx], test['X'][:,:,:,split_idx:]\n",
    "        self.test_y, self.valid_y = test['y'][:split_idx], test['y'][split_idx:]\n",
    "        self.train_x, self.train_y = train['X'], train['y']\n",
    "        # The SVHN dataset comes with lots of labels, but for the purpose of this exercise,\n",
    "        # we will pretend that there are only 1000.\n",
    "        # We use this mask to say which labels we will allow ourselves to use.\n",
    "        self.label_mask = np.zeros_like(self.train_y)\n",
    "        self.label_mask[0:1000] = 1\n",
    "        \n",
    "        self.train_x = np.rollaxis(self.train_x, 3)\n",
    "        self.valid_x = np.rollaxis(self.valid_x, 3)\n",
    "        self.test_x = np.rollaxis(self.test_x, 3)\n",
    "        \n",
    "        if scale_func is None:\n",
    "            self.scaler = scale\n",
    "        else:\n",
    "            self.scaler = scale_func\n",
    "        self.train_x = self.scaler(self.train_x)\n",
    "        self.valid_x = self.scaler(self.valid_x)\n",
    "        self.test_x = self.scaler(self.test_x)\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def batches(self, batch_size, which_set=\"train\"):\n",
    "        x_name = which_set + \"_x\"\n",
    "        y_name = which_set + \"_y\"\n",
    "        \n",
    "        num_examples = len(getattr(dataset, y_name))\n",
    "        if self.shuffle:\n",
    "            idx = np.arange(num_examples)\n",
    "            np.random.shuffle(idx)\n",
    "            setattr(dataset, x_name, getattr(dataset, x_name)[idx])\n",
    "            setattr(dataset, y_name, getattr(dataset, y_name)[idx])\n",
    "            if which_set == \"train\":\n",
    "                dataset.label_mask = dataset.label_mask[idx]\n",
    "        \n",
    "        dataset_x = getattr(dataset, x_name)\n",
    "        dataset_y = getattr(dataset, y_name)\n",
    "        for ii in range(0, num_examples, batch_size):\n",
    "            x = dataset_x[ii:ii+batch_size]\n",
    "            y = dataset_y[ii:ii+batch_size]\n",
    "            \n",
    "            if which_set == \"train\":\n",
    "                # When we use the data for training, we need to include\n",
    "                # the label mask, so we can pretend we don't have access\n",
    "                # to some of the labels, as an exercise of our semi-supervised\n",
    "                # learning ability\n",
    "                yield x, y, self.label_mask[ii:ii+batch_size]\n",
    "            else:\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(\n",
    "        tf.float32,\n",
    "        (None, *real_dim),\n",
    "        name='input_real'\n",
    "    )\n",
    "    inputs_z = tf.placeholder(\n",
    "        tf.float32,\n",
    "        (None, z_dim),\n",
    "        name='input_z'\n",
    "    )\n",
    "    y = tf.placeholder(\n",
    "        tf.int32,\n",
    "        (None),\n",
    "        name='y'\n",
    "    )\n",
    "    label_mask = tf.placeholder(\n",
    "        tf.int32,\n",
    "        (None),\n",
    "        name='label_mask'\n",
    "    )\n",
    "    return inputs_real, inputs_z, y, label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, output_dim, reuse=False, alpha=0.2, training=True, size_mult=128):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First  fully connected layer\n",
    "        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)\n",
    "        # Reshape it to start the convolutional stack\n",
    "        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))\n",
    "        x1 = tf.layers.batch_normalization(x1, training=training)\n",
    "        x1 = tf.maximum(alpha * x1, x1)\n",
    "\n",
    "        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding='same')\n",
    "        x2 = tf.layers.batch_normalization(x2, training=training)\n",
    "        x2 = tf.maximum(alpha * x2, x2)\n",
    "\n",
    "        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='same')\n",
    "        x3 = tf.layers.batch_normalization(x3, training=training)\n",
    "        x3 = tf.maximum(alpha * x3, x3)\n",
    "\n",
    "        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')\n",
    "\n",
    "        return tf.tanh(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2, drop_rate=0., num_classes=10, size_mult=64):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        x = tf.layers.dropout(x, rate=drop_rate / 2.5)\n",
    "        \n",
    "        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')\n",
    "        x1 = tf.maximum(alpha * x1, x1)\n",
    "        x1 = tf.layers.dropout(x1, rate=drop_rate)\n",
    "        \n",
    "        x2 = tf.layers.conv2d(x1, size_mult, 3, strides=2, padding='same')\n",
    "        x2 = tf.layers.batch_normalization(x2, training=True)\n",
    "        x2 = tf.maximum(alpha * x2, x2)\n",
    "        \n",
    "        x3 = tf.layers.conv2d(x2, size_mult, 3, strides=2, padding='same')\n",
    "        x3 = tf.layers.batch_normalization(x3, training=True)\n",
    "        x3 = tf.maximum(alpha * x3, x3)\n",
    "        x3 = tf.layers.dropout(x3, rate=drop_rate)\n",
    "        \n",
    "        x4 = tf.layers.conv2d(x3, 2 * size_mult, 3, strides=1, padding='same')\n",
    "        x4 = tf.layers.batch_normalization(x4, training=True)\n",
    "        x4 = tf.maximum(alpha * x4, x4)\n",
    "        \n",
    "        x5 = tf.layers.conv2d(x4, 2 * size_mult, 3, strides=1, padding='same')\n",
    "        x5 = tf.layers.batch_normalization(x5, training=True)\n",
    "        x5 = tf.maximum(alpha * x5, x5)\n",
    "        \n",
    "        x6 = tf.layers.conv2d(x5, 2 * size_mult, 3, strides=2, padding='same')\n",
    "        x6 = tf.layers.batch_normalization(x6, training=True)\n",
    "        x6 = tf.maximum(alpha * x6, x6)\n",
    "        x6 = tf.layers.dropout(x6, rate=drop_rate)\n",
    "        \n",
    "        x7 = tf.layers.conv2d(x6, 2 * size_mult, 3, strides=1, padding='valid')\n",
    "        x7 = tf.maximum(alpha * x7, x7)\n",
    "        \n",
    "        # Flatten it by global average pooling\n",
    "        features = tf.reduce_mean(x7, (1, 2))\n",
    "        \n",
    "        # Set class_logits to be the inputs to a softmax distribution over the different classes\n",
    "        class_logits = tf.layers.dense(features, num_classes + extra_class)\n",
    "        \n",
    "        if extra_class:\n",
    "            real_class_logits, fake_class_logits = tf.split(class_logits, [num_classes, 1], 1)\n",
    "            assert fake_class_logits.get_shape()[1] == 1, fake_class_logits.get_shape()\n",
    "            fake_class_logits = tf.squeeze(fake_class_logits)\n",
    "        else:\n",
    "            real_class_logits = class_logits\n",
    "            fake_class_logits = 0\n",
    "            \n",
    "        mx = tf.reduce_max(real_class_logits, 1, keep_dims=True)\n",
    "        stable_real_class_logits = real_class_logits - mx\n",
    "        \n",
    "        gan_logits = tf.log(tf.reduce_sum(tf.exp(stable_real_class_logits, 1))) + tf.squeeze(mx) - fake_class_logits\n",
    "        \n",
    "        out = tf.nn.softmax(class_logits)\n",
    "        \n",
    "        return out, class_logits, gan_logits, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_dim, y,\n",
    "    num_classes, label_mask, alpha=0.2, drop_rate=0.0):\n",
    "    \n",
    "    \n",
    "    g_size_mult = 32\n",
    "    d_size_mult = 64\n",
    "    \n",
    "    g_model = generator(input_z, output_dim, alpha=alpha, size_mult=g_size_mult)\n",
    "    \n",
    "    d_on_data = discriminator(input_real, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)\n",
    "    d_model_real, class_logits_on_data, gan_logits_on_data, data_features = d_on_data\n",
    "    \n",
    "    d_on_samples = discriminator(g_model, reuse=True, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)\n",
    "    d_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = d_on_samples\n",
    "    \n",
    "    # Here we compute `d_loss`, the loss for the discriminator.\n",
    "    # This should combine two different losses:\n",
    "    #  1. The loss for the GAN problem, where we minimize the cross-entropy for the binary\n",
    "    #     real-vs-fake classification problem.\n",
    "    #  2. The loss for the SVHN digit classification problem, where we minimize the cross-entropy\n",
    "    #     for the multi-class softmax. For this one we use the labels. Don't forget to ignore\n",
    "    #     use `label_mask` to ignore the examples that we are pretending are unlabeled for the\n",
    "    #     semi-supervised learning problem.\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=gan_logits_on_data,\n",
    "            labels=tf.ones_like(gan_logits_on_data)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=gan_logits_on_samples,\n",
    "            labels=tf.zeros_like(gan_logits_on_samples)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    y = tf.squeeze(y)\n",
    "    \n",
    "    class_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=class_logits_on_data,\n",
    "        labels=tf.one_hot(y, num_classes + extra_class, dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    class_cross_entropy = tf.squeeze(class_cross_entropy)\n",
    "    \n",
    "    label_mask = tf.squeeze(tf.to_float(label_mask))\n",
    "    d_loss_class = tf.reduce_sum(label_mask * class_cross_entropy) / tf.maximum(1.0, tf.reduce_sum(label_mask))\n",
    "    \n",
    "    \n",
    "    d_loss = d_loss_class + d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\n",
    "    # This loss consists of minimizing the absolute difference between the expected features\n",
    "    # on the data and the expected features on the generated samples.\n",
    "    # This loss works better for semi-supervised learning than the tradition GAN losses\n",
    "    \n",
    "    data_moments = tf.reduce_mean(data_features, axis=0)\n",
    "    sample_moments = tf.reduce_mean(sample_features, axis=0)\n",
    "    \n",
    "    g_loss = tf.reduce_mean(tf.abs(data_moments - sample_moments))\n",
    "    \n",
    "    pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)\n",
    "    eq = tf.equal(tf.squeeze(y), pred_class)\n",
    "    correct = tf.reduce_sum(tf.to_float(eq))\n",
    "    masked_correct = tf.reduce_sum(label_ma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
