{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get svhn dataset\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "if not isdir(data_dir):\n",
    "    raise Exception('Data directory does not exist')\n",
    "    \n",
    "    \n",
    "# Custom class to show download progress\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    \n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "        \n",
    "if not isfile(data_dir + 'train_32x32.mat'):\n",
    "    progressBar = DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN train dataset')\n",
    "    urlretrieve(\n",
    "        'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
    "        data_dir + 'train_32x32.mat',\n",
    "        progressBar.hook\n",
    "    )\n",
    "\n",
    "if not isfile(data_dir + 'test_32x32.mat'):\n",
    "    progressBar = DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN test dataset')\n",
    "    urlretrieve(\n",
    "        'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
    "        data_dir + 'test_32x32.mat',\n",
    "        progressBar.hook\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test and train set\n",
    "\n",
    "train_set = loadmat(data_dir + 'train_32x32.mat')\n",
    "test_set = loadmat(data_dir + 'test_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale images to match generator output\n",
    "\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale to (0, 1)\n",
    "    x = ((x - x.min())/(255 - x.min()))\n",
    "    \n",
    "    # scale to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into test and validation sets\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, train, test, val_frac=0.5, shuffle=False, scale_func=None):\n",
    "        split_index = int(len(test['y']) * (1 - val_frac))\n",
    "        \n",
    "        # Test and validation input set\n",
    "        self.test_x = test['X'][:, :, :, :split_index]\n",
    "        self.valid_x = test['X'][:, :, :, split_index:]\n",
    "        \n",
    "        # Test and validation labels set\n",
    "        self.test_y = test['y'][:split_index]\n",
    "        self.valid_y = test['y'][split_index:]\n",
    "        \n",
    "        # Train input and label sets\n",
    "        self.train_x, self.train_y = train['X'], train['y']\n",
    "        \n",
    "        # Roll the specified axis backwards\n",
    "        self.train_x = np.rollaxis(self.train_x, 3)\n",
    "        self.valid_x = np.rollaxis(self.valid_x, 3)\n",
    "        self.test_x = np.rollaxis(self.test_x, 3)\n",
    "        \n",
    "        # Scale\n",
    "        self.scaler = scale_func if scale_func else scale\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def batches(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            index = np.arange(len(dataset.train_x))\n",
    "            np.random.shuffle(index)\n",
    "            self.train_x = self.train_x[index]\n",
    "            self.train_y = self.train_y[index]\n",
    "        \n",
    "        for i in range(0, len(self.train_y), batch_size):\n",
    "            x = self.train_x[i: i + batch_size]\n",
    "            y = self.train_y[i: i + batch_size]\n",
    "            \n",
    "            yield self.scaler(x), self.scaler(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model inputs\n",
    "\n",
    "def model_inputs(discriminator_dim, generator_dim):\n",
    "    input_discriminator = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, discriminator_dim],\n",
    "        name='input_discriminator'\n",
    "    )\n",
    "    \n",
    "    input_generator = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, generator_dim],\n",
    "        name='input_discriminator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "def generator(_input, output_dim, reuse=False, alpha=0.2, is_training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        ## Fully connected layer\n",
    "        fully = tf.layers.dense(_input, 4 * 4 * 512)\n",
    "        # Reshape it to start the convolutional stack\n",
    "        fully = tf.reshape(fully, (-1, 4, 4, 512))\n",
    "        # Apply batch normalization\n",
    "        fully = tf.layers.batch_normalization(fully, trianing=is_training)\n",
    "        # Apply leaky relu\n",
    "        fully = tf.maximum(alpha * fully, fully)\n",
    "        \n",
    "        ## First deconvolution\n",
    "        conv_1 = tf.layers.conv2d_transpose(fully, 256, 5, strides=2, padding='same')\n",
    "        # Apply batch normalization\n",
    "        conv_1 = tf.layers.batch_normalization(conv_1, training=is_training)\n",
    "        # Apply leaky relu\n",
    "        conv_1 = tf.maximum(alpha * conv_1, conv_1)\n",
    "        \n",
    "        ## Second deconvolution\n",
    "        conv_2 = tf.layers.conv2d_transpose(conv_1, 128, 5, strides=2, padding='same')\n",
    "        # Apply batch normalization\n",
    "        conv_2 = tf.layers.batch_normalization(conv_2, training=is_training)\n",
    "        # Apply leaky relu\n",
    "        conv_2 = tf.maximum(alpha * conv_2, conv_2)\n",
    "        \n",
    "        ## Output deconvolution layer\n",
    "        \n",
    "        logits = tf.layers_conv2d_transpose(conv_2, output_dim, 5, strides=2, padding='same')\n",
    "        return tf.tanh(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(_input, reuse=False, alpha=0.2):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        ## First convolution\n",
    "        conv_1 = tf.layers.conv2d(_input, 64, 5, strides=2, padding='same')\n",
    "        # Apply leaky relu without batch normalization\n",
    "        conv_1 = tf.maximum(alpha * conv_1, conv_1)\n",
    "        \n",
    "        ## Second convolution\n",
    "        conv_2 = tf.layers.conv2d(conv_1, 128, 5, strides=2, padding='same')\n",
    "        # Apply batch normalization\n",
    "        conv_2 = tf.layers.batch_normalization(conv_2, training=True)\n",
    "        # Apply leaky relu\n",
    "        conv_2 = tf.maximum(alpha * conv_2, conv_2)\n",
    "        \n",
    "        ## Third convolution\n",
    "        conv_3 = tf.layers.conv2d(conv_2, 256, 5, strides=2, padding='same')\n",
    "        # Apply batch normalization\n",
    "        conv_3 = tf.layers.batch_normalization(conv_3, training=True)\n",
    "        # Apply leaky relu\n",
    "        conv_3 = tf.maximum(alpha * conv_3, conv_3)\n",
    "        \n",
    "        ## Output fully connected layer and flatten it\n",
    "        output = tf.reshape(conv_3, (-1, 4 * 4 * 256))\n",
    "        logits = tf.layers.dense(output, 1)\n",
    "        output = tf.sigmoid(logits)\n",
    "        \n",
    "        return output, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model loss\n",
    "\n",
    "def model_loss(input_discriminator, input_generator, output_dime, alpha=0.2):\n",
    "    ## Create genereator model\n",
    "    generator_model = generator(\n",
    "        input_generator,\n",
    "        output_dim,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    \n",
    "    ## Create discriminator real model\n",
    "    discriminator_model_real, discriminator_logits_real = discriminator(\n",
    "        input_discriminator,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    \n",
    "    ## Create discriminator fake model\n",
    "    discriminator_model_fake, discriminator_logits_fake = discriminator(\n",
    "        generator_model,\n",
    "        reuse=True,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    \n",
    "    ## Get discriminator real loss\n",
    "    discriminator_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=discriminator_logits_real,\n",
    "        labels=tf.ones_like(discriminator_model_real)\n",
    "    )\n",
    "    discriminator_loss_real = tf.reduce_mean(discriminator_loss_real)\n",
    "    \n",
    "    ## Get discriminator fake loss\n",
    "    discriminator_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=discriminator_logits_fake,\n",
    "        labels=tf.zeros_like(discriminator_model_fake)\n",
    "    )\n",
    "    discriminator_loss_fake = tf.reduce_mean(discriminator_loss_fake)\n",
    "    \n",
    "    ## Add discriminator losses\n",
    "    discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "    \n",
    "    ## Get generator loss\n",
    "    generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=discriminator_logits_fake,\n",
    "        labels=tf.ones_like(discriminator_model_fake)\n",
    "    )\n",
    "    generator_loss = tf.reduce_mean(generator_loss)\n",
    "    \n",
    "    return discriminator_loss, generator_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "def model_optimizers(discriminator_loss, generator_loss, learning_rate, beta):\n",
    "    ## Split trainable vars into generator and discriminator vars\n",
    "    training_vars = tf.trainable_variables()\n",
    "    discriminator_vars = [var for var in training_vars if var.name.startswith('discriminator')]\n",
    "    generator_vars = [var for var in training_vars if var.name.startswith('generator')]\n",
    "    \n",
    "    ## Optimize\n",
    "    \n",
    "    with tf.control_dependecies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        discriminator_train = tf.train.AdamOptimizer(\n",
    "            learning_rate,\n",
    "            beta1=beta\n",
    "        ).minimize(discriminator_loss, var_list=discriminator_vars)\n",
    "        \n",
    "        generator_train = tf.train.AdamOptimizer(\n",
    "            learning_rate,\n",
    "            beta1=beta\n",
    "        ).minimize(generator_loss, var_list=generator_vars)\n",
    "    \n",
    "    return discriminator_train, generator_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
