{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get svhn dataset\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "if not isdir(data_dir):\n",
    "    raise Exception('Data directory does not exist')\n",
    "    \n",
    "    \n",
    "# Custom class to show download progress\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    \n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "        \n",
    "if not isfile(data_dir + 'train_32x32.mat'):\n",
    "    progressBar = DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN train dataset')\n",
    "    urlretrieve(\n",
    "        'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
    "        data_dir + 'train_32x32.mat',\n",
    "        progressBar.hook\n",
    "    )\n",
    "\n",
    "if not isfile(data_dir + 'test_32x32.mat'):\n",
    "    progressBar = DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN test dataset')\n",
    "    urlretrieve(\n",
    "        'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
    "        data_dir + 'test_32x32.mat',\n",
    "        progressBar.hook\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test and train set\n",
    "\n",
    "train_set = loadmat(data_dir + 'train_32x32.mat')\n",
    "test_set = loadmat(data_dir + 'test_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale images to match generator output\n",
    "\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale to (0, 1)\n",
    "    x = ((x - x.min())/(255 - x.min()))\n",
    "    \n",
    "    # scale to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into test and validation sets\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, train, test, val_frac=0.5, shuffle=False, scale_func=None):\n",
    "        split_index = int(len(test['y']) * (1 - val_frac))\n",
    "        \n",
    "        # Test and validation input set\n",
    "        self.test_x = test['X'][:, :, :, :split_index]\n",
    "        self.valid_x = test['X'][:, :, :, split_index:]\n",
    "        \n",
    "        # Test and validation labels set\n",
    "        self.test_y = test['y'][:split_index]\n",
    "        self.valid_y = test['y'][split_index:]\n",
    "        \n",
    "        # Train input and label sets\n",
    "        self.train_x, self.train_y = train['X'], train['y']\n",
    "        \n",
    "        # Roll the specified axis backwards\n",
    "        self.train_x = np.rollaxis(self.train_x, 3)\n",
    "        self.valid_x = np.rollaxis(self.valid_x, 3)\n",
    "        self.test_x = np.rollaxis(self.test_x, 3)\n",
    "        \n",
    "        # Scale\n",
    "        self.scaler = scale_func if scale_func else scale\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def batches(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            index = np.arange(len(dataset.train_x))\n",
    "            np.random.shuffle(index)\n",
    "            self.train_x = self.train_x[index]\n",
    "            self.train_y = self.train_y[index]\n",
    "        \n",
    "        for i in range(0, len(self.train_y), batch_size):\n",
    "            x = self.train_x[i: i + batch_size]\n",
    "            y = self.train_y[i: i + batch_size]\n",
    "            \n",
    "            yield self.scaler(x), self.scaler(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model inputs\n",
    "\n",
    "def model_inputs(discriminator_dim, generator_dim):\n",
    "    input_discriminator = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, discriminator_dim],\n",
    "        name='input_discriminator'\n",
    "    )\n",
    "    \n",
    "    input_generator = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, generator_dim],\n",
    "        name='input_discriminator'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "def generator(_input, output_dim, reuse=False, alpha=0.2, is_training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        ## Fully connected layer\n",
    "        fully = tf.layers.dense(_input, 4 * 4 * 512)\n",
    "        # Reshape it to start the convolutional stack\n",
    "        fully = tf.reshape(fully, (-1, 4, 4, 512))\n",
    "        # Apply batch normalization\n",
    "        fully = tf.layers.batch_normalization(fully, trianing=is_training)\n",
    "        # Apply leaky relu\n",
    "        fully = tf.maximum(alpha * fully, fully)\n",
    "        \n",
    "        ## First deconvolution\n",
    "        conv_1 = tf.layers.conv2d_transpose(fully, 256, 5, strides=2, padding='same')\n",
    "        # Apply batch normalization\n",
    "        conv_1 = tf.layers.batch_normalization(conv_1, training=is_training)\n",
    "        # Apply leaky relu\n",
    "        conv_1 = tf.maximum(alpha * conv_1, conv_1)\n",
    "        \n",
    "        ## Second deconvolution\n",
    "        conv_2 = tf.layers.conv2d_transpose(conv_1, 128, 5, strides=2, padding='same')\n",
    "        # Apply batch normalization\n",
    "        conv_2 = tf.layers.batch_normalization(conv_2, training=is_training)\n",
    "        # Apply leaky relu\n",
    "        conv_2 = tf.maximum(alpha * conv_2, conv_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
